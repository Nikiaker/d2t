2025-12-10 23:08:12,783 - openevolve.controller - INFO - Logging to openevolve_output/logs/openevolve_20251210_230812.log
2025-12-10 23:08:12,791 - openevolve.controller - INFO - Set random seed to 42 for reproducibility
2025-12-10 23:08:12,847 - openevolve.llm.openai - INFO - Initialized OpenAI LLM with model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:08:12,847 - openevolve.llm.ensemble - INFO - Initialized LLM ensemble with models: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic (weight: 1.00)
2025-12-10 23:08:12,853 - openevolve.prompt.sampler - INFO - Initialized prompt sampler
2025-12-10 23:08:12,854 - openevolve.prompt.sampler - INFO - Set custom templates: system=evaluator_system_message, user=None
2025-12-10 23:08:12,854 - openevolve.database - INFO - Initialized program database with 0 programs
2025-12-10 23:08:14,182 - openevolve.evaluator - INFO - Successfully loaded evaluation function from evaluator.py
2025-12-10 23:08:14,182 - openevolve.evaluator - WARNING - Configuration has 'cascade_evaluation: true' but evaluator 'evaluator.py' does not define 'evaluate_stage1' function. This will fall back to direct evaluation, making the cascade setting useless. Consider setting 'cascade_evaluation: false' or implementing cascade functions.
2025-12-10 23:08:14,182 - openevolve.evaluator - INFO - Initialized evaluator with evaluator.py
2025-12-10 23:08:14,182 - openevolve.controller - INFO - Initialized OpenEvolve with initial_program.py
2025-12-10 23:08:14,182 - openevolve.controller - INFO - Adding initial program to database
2025-12-10 23:08:14,962 - openevolve.evaluator - INFO - Evaluated program ade40270-10a4-40e0-bfdd-14c641c23ebe in 0.78s: combined_score=0.2514
2025-12-10 23:08:14,962 - openevolve.database - INFO - New MAP-Elites cell occupied in island 0: {'complexity': 5, 'diversity': 0}
2025-12-10 23:08:14,963 - openevolve.process_parallel - INFO - Initialized process parallel controller with 3 workers
2025-12-10 23:08:14,964 - openevolve.process_parallel - INFO - Started process pool with 3 processes
2025-12-10 23:08:14,964 - openevolve.controller - INFO - Using island-based evolution with 1 islands
2025-12-10 23:08:14,964 - openevolve.database - INFO - Island Status:
2025-12-10 23:08:14,964 - openevolve.database - INFO -  * Island 0: 1 programs, best=0.2514, avg=0.2514, diversity=0.00, gen=0 (best: ade40270-10a4-40e0-bfdd-14c641c23ebe)
2025-12-10 23:08:14,964 - openevolve.process_parallel - INFO - Starting process-based evolution from iteration 1 for 100 iterations (total: 101)
2025-12-10 23:08:14,974 - openevolve.process_parallel - INFO - Early stopping disabled
2025-12-10 23:08:14,998 - openevolve.prompt.sampler - INFO - Set custom templates: system=evaluator_system_message, user=None
2025-12-10 23:08:15,013 - openevolve.prompt.sampler - INFO - Set custom templates: system=evaluator_system_message, user=None
2025-12-10 23:08:15,043 - openevolve.prompt.sampler - INFO - Set custom templates: system=evaluator_system_message, user=None
2025-12-10 23:08:17,715 - openevolve.evaluator - INFO - Successfully loaded evaluation function from evaluator.py
2025-12-10 23:08:17,716 - openevolve.evaluator - WARNING - Configuration has 'cascade_evaluation: true' but evaluator 'evaluator.py' does not define 'evaluate_stage1' function. This will fall back to direct evaluation, making the cascade setting useless. Consider setting 'cascade_evaluation: false' or implementing cascade functions.
2025-12-10 23:08:17,716 - openevolve.evaluator - INFO - Initialized evaluator with evaluator.py
2025-12-10 23:08:17,716 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:08:17,852 - openevolve.evaluator - INFO - Successfully loaded evaluation function from evaluator.py
2025-12-10 23:08:17,852 - openevolve.evaluator - WARNING - Configuration has 'cascade_evaluation: true' but evaluator 'evaluator.py' does not define 'evaluate_stage1' function. This will fall back to direct evaluation, making the cascade setting useless. Consider setting 'cascade_evaluation: false' or implementing cascade functions.
2025-12-10 23:08:17,852 - openevolve.evaluator - INFO - Initialized evaluator with evaluator.py
2025-12-10 23:08:17,853 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:08:17,856 - openevolve.evaluator - INFO - Successfully loaded evaluation function from evaluator.py
2025-12-10 23:08:17,857 - openevolve.evaluator - WARNING - Configuration has 'cascade_evaluation: true' but evaluator 'evaluator.py' does not define 'evaluate_stage1' function. This will fall back to direct evaluation, making the cascade setting useless. Consider setting 'cascade_evaluation: false' or implementing cascade functions.
2025-12-10 23:08:17,857 - openevolve.evaluator - INFO - Initialized evaluator with evaluator.py
2025-12-10 23:08:17,859 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:09:38,616 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 23:09:38,616 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 23:09:38,625 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:09:38,626 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:09:38,634 - openevolve.process_parallel - WARNING - Iteration 1 error: No valid diffs found in response
2025-12-10 23:09:38,635 - openevolve.process_parallel - WARNING - Iteration 2 error: No valid diffs found in response
2025-12-10 23:09:46,897 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 23:09:48,800 - openevolve.evaluator - INFO - Evaluated program 9006210d-00ef-4b0e-9fd6-4f62acf62563 in 1.89s: combined_score=0.5533
2025-12-10 23:09:48,801 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:09:48,806 - openevolve.database - INFO - New MAP-Elites cell occupied in island 0: {'complexity': 9, 'diversity': 5}
2025-12-10 23:09:48,806 - openevolve.database - INFO - New best program 9006210d-00ef-4b0e-9fd6-4f62acf62563 replaces ade40270-10a4-40e0-bfdd-14c641c23ebe (combined_score: 0.2514 â†’ 0.5533, +0.3019)
2025-12-10 23:09:48,806 - openevolve.process_parallel - INFO - Iteration 3: Program 9006210d-00ef-4b0e-9fd6-4f62acf62563 (parent: ade40270-10a4-40e0-bfdd-14c641c23ebe) completed in 91.08s
2025-12-10 23:09:48,806 - openevolve.process_parallel - INFO - Metrics: combined_score=0.5533
2025-12-10 23:09:48,807 - openevolve.process_parallel - INFO - ðŸŒŸ New best solution found at iteration 3: 9006210d-00ef-4b0e-9fd6-4f62acf62563
2025-12-10 23:11:04,438 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 23:11:04,493 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 23:11:07,346 - openevolve.evaluator - INFO - Evaluated program c8a0ad69-4996-4102-88b8-f4850743a6f9 in 2.84s: combined_score=0.5533
2025-12-10 23:11:07,347 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:11:07,352 - openevolve.process_parallel - INFO - Iteration 4: Program c8a0ad69-4996-4102-88b8-f4850743a6f9 (parent: ade40270-10a4-40e0-bfdd-14c641c23ebe) completed in 88.72s
2025-12-10 23:11:07,352 - openevolve.process_parallel - INFO - Metrics: combined_score=0.5533
2025-12-10 23:11:07,421 - openevolve.evaluator - INFO - Evaluated program cd3d90b6-3a8b-4a36-8523-33c20b554325 in 2.98s: combined_score=0.5533
2025-12-10 23:11:07,422 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:11:07,423 - openevolve.process_parallel - INFO - Iteration 5: Program cd3d90b6-3a8b-4a36-8523-33c20b554325 (parent: ade40270-10a4-40e0-bfdd-14c641c23ebe) completed in 88.80s
2025-12-10 23:11:07,423 - openevolve.process_parallel - INFO - Metrics: combined_score=0.5533
2025-12-10 23:11:15,583 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 23:11:17,313 - openevolve.evaluator - INFO - Evaluated program 9f9bdc7a-de44-46d8-8ada-b087d27daa6a in 1.73s: combined_score=0.5533
2025-12-10 23:11:17,314 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:11:17,319 - openevolve.process_parallel - INFO - Iteration 6: Program 9f9bdc7a-de44-46d8-8ada-b087d27daa6a (parent: ade40270-10a4-40e0-bfdd-14c641c23ebe) completed in 88.51s
2025-12-10 23:11:17,319 - openevolve.process_parallel - INFO - Metrics: combined_score=0.5533
2025-12-10 23:12:33,083 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 23:12:34,812 - openevolve.evaluator - INFO - Evaluated program 0a17210e-bad1-4a95-955e-1202a41f2f15 in 1.73s: combined_score=0.5529
2025-12-10 23:12:34,816 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:12:34,827 - openevolve.database - INFO - New MAP-Elites cell occupied in island 0: {'complexity': 9, 'diversity': 0}
2025-12-10 23:12:34,828 - openevolve.process_parallel - INFO - Iteration 8: Program 0a17210e-bad1-4a95-955e-1202a41f2f15 (parent: ade40270-10a4-40e0-bfdd-14c641c23ebe) completed in 87.39s
2025-12-10 23:12:34,828 - openevolve.process_parallel - INFO - Metrics: combined_score=0.5529
2025-12-10 23:12:34,883 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:34,884 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 10461 input tokens (20000 > 30000 - 10461). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:37,378 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 23:12:39,086 - openevolve.evaluator - INFO - Evaluated program bc31d22c-2270-4b60-a8cc-00c51d355402 in 1.71s: combined_score=0.5533
2025-12-10 23:12:39,087 - openevolve.database - INFO - New MAP-Elites cell occupied in island 0: {'complexity': 9, 'diversity': 9}
2025-12-10 23:12:39,088 - openevolve.process_parallel - INFO - Iteration 7: Program bc31d22c-2270-4b60-a8cc-00c51d355402 (parent: ade40270-10a4-40e0-bfdd-14c641c23ebe) completed in 91.74s
2025-12-10 23:12:39,088 - openevolve.process_parallel - INFO - Metrics: combined_score=0.5533
2025-12-10 23:12:39,090 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:12:39,141 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:39,141 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13486 input tokens (20000 > 30000 - 13486). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:39,928 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:39,928 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 10461 input tokens (20000 > 30000 - 10461). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:43,921 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 23:12:43,923 - openevolve.process_parallel - WARNING - Iteration 9 error: No valid diffs found in response
2025-12-10 23:12:43,924 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:12:43,959 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:43,960 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16394 input tokens (20000 > 30000 - 16394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:44,200 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:44,201 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13486 input tokens (20000 > 30000 - 13486). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:44,991 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:44,992 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 10461 input tokens (20000 > 30000 - 10461). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:49,021 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:49,021 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16394 input tokens (20000 > 30000 - 16394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:49,265 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:49,266 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13486 input tokens (20000 > 30000 - 13486). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:50,054 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:50,055 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 10461 input tokens (20000 > 30000 - 10461). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:12:50,056 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 10461 input tokens (20000 > 30000 - 10461). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:12:50,057 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:12:50,060 - openevolve.process_parallel - WARNING - Iteration 10 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 10461 input tokens (20000 > 30000 - 10461). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:12:50,087 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:50,087 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13934 input tokens (20000 > 30000 - 13934). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:54,087 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:54,088 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16394 input tokens (20000 > 30000 - 16394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:54,328 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:54,329 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13486 input tokens (20000 > 30000 - 13486). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:12:54,330 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13486 input tokens (20000 > 30000 - 13486). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:12:54,331 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:12:54,339 - openevolve.process_parallel - WARNING - Iteration 11 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13486 input tokens (20000 > 30000 - 13486). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:12:54,365 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:54,365 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:55,143 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:55,144 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13934 input tokens (20000 > 30000 - 13934). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:59,167 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:59,168 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16394 input tokens (20000 > 30000 - 16394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:12:59,169 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16394 input tokens (20000 > 30000 - 16394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:12:59,171 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:12:59,176 - openevolve.process_parallel - WARNING - Iteration 12 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16394 input tokens (20000 > 30000 - 16394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:12:59,206 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:59,207 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:12:59,426 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:12:59,427 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:00,208 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:00,208 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13934 input tokens (20000 > 30000 - 13934). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:04,270 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:04,271 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:04,495 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:04,496 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:05,270 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:05,271 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13934 input tokens (20000 > 30000 - 13934). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:05,272 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13934 input tokens (20000 > 30000 - 13934). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:05,274 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:05,275 - openevolve.process_parallel - WARNING - Iteration 13 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13934 input tokens (20000 > 30000 - 13934). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:05,309 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:05,309 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:09,345 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:09,346 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:09,566 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:09,567 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:09,568 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:09,569 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:09,572 - openevolve.process_parallel - WARNING - Iteration 14 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:09,604 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:09,604 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:10,374 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:10,375 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:14,418 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:14,419 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:14,420 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:14,421 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:14,429 - openevolve.process_parallel - WARNING - Iteration 15 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:14,458 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:14,458 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:14,676 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:14,677 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:15,448 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:15,449 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:19,538 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:19,539 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:19,749 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:19,749 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:20,516 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:20,517 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:20,518 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:20,519 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:20,528 - openevolve.process_parallel - WARNING - Iteration 16 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:20,554 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:20,555 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:24,611 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:24,612 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:24,820 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:24,821 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:24,822 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:24,824 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:24,825 - openevolve.process_parallel - WARNING - Iteration 17 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:24,858 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:24,858 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:25,625 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:25,625 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:29,686 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:29,687 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:29,688 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:29,689 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:29,693 - openevolve.process_parallel - WARNING - Iteration 18 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:29,724 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:29,724 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:29,922 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:29,923 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:30,693 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:30,694 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:34,788 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:34,789 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:34,994 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:34,994 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:35,770 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:35,771 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:35,772 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:35,773 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:35,776 - openevolve.process_parallel - WARNING - Iteration 19 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:35,811 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:35,811 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:39,860 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:39,860 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:40,065 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:40,066 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:40,067 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:40,068 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:40,075 - openevolve.process_parallel - WARNING - Iteration 20 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:40,098 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:40,099 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:40,874 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:40,874 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:44,933 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:44,933 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:44,934 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:44,937 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:44,940 - openevolve.process_parallel - WARNING - Iteration 21 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:44,972 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:44,972 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:45,155 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:45,156 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:45,947 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:45,947 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:50,043 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:50,044 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:50,218 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:50,219 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:51,030 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:51,031 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:51,031 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:51,033 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:51,033 - openevolve.process_parallel - WARNING - Iteration 22 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:51,063 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:51,063 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:55,124 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:55,125 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:55,282 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:55,283 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:55,284 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:55,287 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:13:55,293 - openevolve.process_parallel - WARNING - Iteration 23 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:13:55,324 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:55,324 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:13:56,118 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:13:56,119 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:00,195 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:00,196 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:00,197 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:00,198 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:00,205 - openevolve.process_parallel - WARNING - Iteration 24 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:00,230 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:00,230 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:00,392 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:00,393 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:01,180 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:01,181 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:05,294 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:05,294 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:05,460 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:05,461 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:06,241 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:06,242 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:06,243 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:06,245 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:06,248 - openevolve.process_parallel - WARNING - Iteration 25 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:06,280 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:06,280 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:10,358 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:10,359 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:10,542 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:10,543 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:10,544 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:10,546 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:10,550 - openevolve.process_parallel - WARNING - Iteration 26 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:10,581 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:10,582 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:11,356 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:11,357 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:15,425 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:15,426 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:15,427 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:15,428 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:15,437 - openevolve.process_parallel - WARNING - Iteration 27 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:15,458 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:15,458 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:15,645 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:15,646 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:16,439 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:16,440 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:20,522 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:20,523 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:20,718 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:20,718 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:21,512 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:21,513 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:21,514 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:21,515 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:21,517 - openevolve.process_parallel - WARNING - Iteration 28 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:21,546 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:21,546 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:25,586 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:25,587 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:25,801 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:25,802 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:25,803 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:25,805 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:25,811 - openevolve.process_parallel - WARNING - Iteration 29 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:25,838 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:25,839 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:26,602 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:26,603 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:30,662 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:30,663 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:30,663 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:30,665 - openevolve.process_parallel - WARNING - Iteration 30 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:30,665 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:30,698 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:30,699 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:30,898 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:30,899 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:31,666 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:31,667 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:35,760 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:35,761 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:35,974 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:35,975 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:36,730 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:36,732 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:36,732 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:36,734 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:36,740 - openevolve.process_parallel - WARNING - Iteration 31 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:36,766 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:36,767 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:40,831 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:40,832 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:41,044 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:41,045 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:41,045 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:41,047 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:41,053 - openevolve.process_parallel - WARNING - Iteration 32 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:41,081 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:41,081 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:41,829 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:41,829 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:45,903 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:45,904 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:45,905 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:45,907 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:45,910 - openevolve.process_parallel - WARNING - Iteration 33 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:45,942 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:45,942 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:46,150 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:46,151 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:46,900 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:46,901 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:51,014 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:51,015 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:51,220 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:51,221 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:51,972 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:51,973 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:51,973 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:51,974 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:51,981 - openevolve.process_parallel - WARNING - Iteration 34 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:52,005 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:52,005 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:56,087 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:56,088 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:56,287 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:56,288 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:56,289 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:56,290 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:14:56,292 - openevolve.process_parallel - WARNING - Iteration 35 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:14:56,324 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:56,325 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:14:57,068 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:14:57,068 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:01,158 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:01,159 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:01,160 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:01,161 - openevolve.process_parallel - WARNING - Iteration 36 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:01,162 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:01,195 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:01,195 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:01,397 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:01,397 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:02,132 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:02,132 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:06,235 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:06,236 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:06,478 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:06,478 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:07,194 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:07,194 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:07,195 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:07,196 - openevolve.process_parallel - WARNING - Iteration 37 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:07,196 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:07,231 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:07,231 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:11,302 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:11,302 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:11,549 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:11,550 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:11,551 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:11,553 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:11,561 - openevolve.process_parallel - WARNING - Iteration 38 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:11,587 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:11,587 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:12,294 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:12,294 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:16,371 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:16,372 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:16,373 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:16,375 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:16,381 - openevolve.process_parallel - WARNING - Iteration 39 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:16,424 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:16,425 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:16,643 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:16,643 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:17,367 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:17,368 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:21,490 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:21,491 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:21,714 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:21,715 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:22,438 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:22,439 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:22,439 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:22,441 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:22,447 - openevolve.process_parallel - WARNING - Iteration 40 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:22,475 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:22,475 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:26,564 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:26,565 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:26,783 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:26,783 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:26,784 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:26,786 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:26,787 - openevolve.process_parallel - WARNING - Iteration 41 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:26,816 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:26,816 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:27,543 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:27,544 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:31,637 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:31,638 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:31,639 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:31,641 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:31,644 - openevolve.process_parallel - WARNING - Iteration 42 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:31,675 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:31,675 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:31,852 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:31,852 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:32,612 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:32,613 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:36,738 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:36,738 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:36,917 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:36,917 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:37,681 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:37,682 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:37,683 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:37,685 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:37,692 - openevolve.process_parallel - WARNING - Iteration 43 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:37,719 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:37,720 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:41,812 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:41,813 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:41,983 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:41,984 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:41,985 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:41,987 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:41,993 - openevolve.process_parallel - WARNING - Iteration 44 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:42,020 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:42,021 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:42,783 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:42,784 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:46,887 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:46,888 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:46,889 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:46,890 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:46,890 - openevolve.process_parallel - WARNING - Iteration 45 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:46,921 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:46,921 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:47,087 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:47,088 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:47,855 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:47,855 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:51,996 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:51,996 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:52,167 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:52,168 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:52,927 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:52,928 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:52,928 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:52,929 - openevolve.process_parallel - WARNING - Iteration 46 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:52,930 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:52,964 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:52,965 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:57,061 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:57,062 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:57,252 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:57,253 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:57,254 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:57,256 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:15:57,261 - openevolve.process_parallel - WARNING - Iteration 47 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:15:57,291 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:57,291 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:15:58,026 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:15:58,027 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:02,128 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:02,129 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:02,129 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:02,132 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:02,138 - openevolve.process_parallel - WARNING - Iteration 48 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:02,164 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:02,164 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:02,359 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:02,360 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:03,098 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:03,099 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:07,237 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:07,238 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:07,440 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:07,440 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:08,172 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:08,173 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:08,174 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:08,176 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:08,177 - openevolve.process_parallel - WARNING - Iteration 49 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:08,211 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:08,212 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:12,305 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:12,305 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:12,522 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:12,523 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:12,524 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:12,525 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:12,526 - openevolve.process_parallel - WARNING - Iteration 50 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:12,560 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:12,560 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:13,283 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:13,284 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:17,371 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:17,372 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:17,373 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:17,374 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:17,383 - openevolve.process_parallel - WARNING - Iteration 51 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13932 input tokens (20000 > 30000 - 13932). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:17,405 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:17,405 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:17,623 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:17,623 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:18,357 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:18,357 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:22,460 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:22,460 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:22,706 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:22,707 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:23,430 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:23,431 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:23,432 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:23,434 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:23,434 - openevolve.process_parallel - WARNING - Iteration 52 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:23,469 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:23,469 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:27,526 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:27,527 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:27,790 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:27,791 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:27,792 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:27,794 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:27,797 - openevolve.process_parallel - WARNING - Iteration 53 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:27,830 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:27,830 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:28,534 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:28,535 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:32,592 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:32,593 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:32,594 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:32,595 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:32,602 - openevolve.process_parallel - WARNING - Iteration 54 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:32,625 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:32,625 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:32,893 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:32,893 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:33,606 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:33,607 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:37,680 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:37,681 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:37,977 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:37,978 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:38,681 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:38,682 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:38,682 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:38,684 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:38,692 - openevolve.process_parallel - WARNING - Iteration 55 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:38,718 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:38,718 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:42,744 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:42,744 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:43,054 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:43,055 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:43,056 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:43,058 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:43,061 - openevolve.process_parallel - WARNING - Iteration 56 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:43,091 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:43,091 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:43,780 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:43,781 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:47,809 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:47,809 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:47,810 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:47,812 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:47,816 - openevolve.process_parallel - WARNING - Iteration 57 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:47,847 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:47,847 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:48,158 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:48,159 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:48,850 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:48,851 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:52,910 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:52,910 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:53,230 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:53,231 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:53,921 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:53,922 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:53,923 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:53,925 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:53,932 - openevolve.process_parallel - WARNING - Iteration 58 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:53,959 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:53,959 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:57,983 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:57,984 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:58,299 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:58,300 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:58,301 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:58,303 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:16:58,310 - openevolve.process_parallel - WARNING - Iteration 59 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:16:58,338 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:58,339 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:16:59,008 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:16:59,009 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:03,055 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:03,055 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:03,056 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:03,057 - openevolve.process_parallel - WARNING - Iteration 60 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:03,059 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:03,093 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:03,093 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:03,409 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:03,410 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:04,082 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:04,082 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:08,154 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:08,154 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:08,493 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:08,494 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:09,153 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:09,153 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:09,154 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:09,156 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:09,166 - openevolve.process_parallel - WARNING - Iteration 61 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:09,189 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:09,190 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:13,228 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:13,229 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:13,567 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:13,567 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:13,568 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:13,570 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:13,575 - openevolve.process_parallel - WARNING - Iteration 62 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:13,605 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:13,605 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:14,261 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:14,262 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:18,298 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:18,299 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:18,299 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:18,301 - openevolve.process_parallel - WARNING - Iteration 63 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:18,301 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:18,335 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:18,336 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:18,653 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:18,654 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:19,337 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:19,337 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:23,406 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:23,407 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:23,725 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:23,726 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:24,416 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:24,417 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:24,418 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:24,420 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:24,421 - openevolve.process_parallel - WARNING - Iteration 64 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:24,455 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:24,455 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:28,475 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:28,476 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:28,790 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:28,791 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:28,792 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:28,793 - openevolve.process_parallel - WARNING - Iteration 65 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:28,794 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:28,827 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:28,827 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:29,519 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:29,519 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:33,549 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:33,549 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:33,550 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:33,552 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:33,559 - openevolve.process_parallel - WARNING - Iteration 66 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:33,586 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:33,586 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:33,889 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:33,890 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:34,591 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:34,592 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:38,649 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:38,650 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:38,963 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:38,963 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:39,662 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:39,663 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:39,664 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:39,665 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:39,668 - openevolve.process_parallel - WARNING - Iteration 67 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:39,699 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:39,700 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:43,723 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:43,724 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:44,031 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:44,032 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:44,033 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:44,035 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:44,041 - openevolve.process_parallel - WARNING - Iteration 68 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:44,070 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:44,070 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:44,776 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:44,777 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:48,798 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:48,799 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:48,799 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:48,801 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:48,809 - openevolve.process_parallel - WARNING - Iteration 69 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:48,835 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:48,835 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:49,141 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:49,142 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:49,853 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:49,854 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:53,905 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:53,906 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:54,211 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:54,212 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:54,925 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:54,926 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:54,927 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:54,929 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:54,930 - openevolve.process_parallel - WARNING - Iteration 70 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:54,964 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:54,964 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:58,963 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:58,964 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:17:59,282 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:59,282 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:59,283 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:59,285 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:17:59,291 - openevolve.process_parallel - WARNING - Iteration 71 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:17:59,320 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:17:59,320 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:00,028 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:00,029 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:04,036 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:04,037 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:04,038 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:04,039 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:04,047 - openevolve.process_parallel - WARNING - Iteration 72 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:04,074 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:04,074 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:04,385 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:04,385 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:05,099 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:05,100 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:09,139 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:09,139 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:09,457 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:09,458 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:10,170 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:10,171 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:10,172 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:10,174 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:10,177 - openevolve.process_parallel - WARNING - Iteration 73 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:10,208 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:10,208 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:14,211 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:14,212 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:14,544 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:14,544 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:14,545 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:14,547 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:14,552 - openevolve.process_parallel - WARNING - Iteration 74 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:14,582 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:14,582 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:15,271 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:15,271 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:19,293 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:19,294 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:19,295 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:19,297 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:19,297 - openevolve.process_parallel - WARNING - Iteration 75 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:19,331 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:19,331 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:19,643 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:19,644 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:20,340 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:20,341 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:24,393 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:24,394 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:24,711 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:24,712 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:25,412 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:25,413 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:25,413 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:25,416 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:25,416 - openevolve.process_parallel - WARNING - Iteration 76 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:25,449 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:25,449 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:29,466 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:29,466 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:29,782 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:29,783 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:29,784 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:29,785 - openevolve.process_parallel - WARNING - Iteration 77 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:29,786 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:29,822 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:29,822 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:30,484 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:30,485 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:34,544 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:34,545 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:34,546 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:34,548 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:34,555 - openevolve.process_parallel - WARNING - Iteration 78 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:34,583 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:34,583 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:34,861 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:34,862 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:35,566 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:35,567 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:39,648 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:39,649 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:39,934 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:39,935 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:40,637 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:40,638 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:40,639 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:40,642 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:40,643 - openevolve.process_parallel - WARNING - Iteration 79 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:40,675 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:40,675 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:44,723 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:44,724 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:45,008 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:45,009 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:45,010 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:45,013 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:45,015 - openevolve.process_parallel - WARNING - Iteration 80 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17791 input tokens (20000 > 30000 - 17791). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:45,046 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:45,047 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:45,744 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:45,745 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:49,809 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:49,812 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:49,813 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:49,815 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:49,822 - openevolve.process_parallel - WARNING - Iteration 81 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:49,848 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:49,848 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:50,107 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:50,108 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:50,812 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:50,813 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:54,909 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:54,909 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:55,178 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:55,179 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:55,882 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:55,882 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:55,883 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:55,885 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:18:55,891 - openevolve.process_parallel - WARNING - Iteration 82 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:18:55,920 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:55,920 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:18:59,982 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:18:59,983 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:00,247 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:00,248 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:00,249 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:00,251 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:00,253 - openevolve.process_parallel - WARNING - Iteration 83 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:00,286 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:00,286 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:00,991 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:00,992 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:05,064 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:05,065 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:05,066 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:05,068 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:05,069 - openevolve.process_parallel - WARNING - Iteration 84 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:05,101 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:05,101 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:05,338 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:05,339 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:06,072 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:06,072 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:10,164 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:10,165 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:10,410 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:10,411 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:11,142 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:11,143 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:11,144 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:11,145 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:11,148 - openevolve.process_parallel - WARNING - Iteration 85 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:11,179 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:11,179 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:15,235 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:15,236 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:15,499 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:15,500 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:15,501 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:15,503 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:15,504 - openevolve.process_parallel - WARNING - Iteration 86 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17792 input tokens (20000 > 30000 - 17792). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:15,537 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:15,537 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:16,241 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:16,241 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:20,305 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:20,306 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:20,307 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:20,308 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:20,308 - openevolve.process_parallel - WARNING - Iteration 87 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:20,338 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:20,338 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:20,598 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:20,599 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:21,312 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:21,313 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:25,406 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:25,406 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:25,676 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:25,677 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:26,379 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:26,380 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:26,381 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:26,383 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:26,387 - openevolve.process_parallel - WARNING - Iteration 88 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:26,417 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:26,417 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:30,472 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:30,473 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:30,743 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:30,744 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:30,744 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:30,746 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:30,749 - openevolve.process_parallel - WARNING - Iteration 89 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16396 input tokens (20000 > 30000 - 16396). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:30,779 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:30,779 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:31,484 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:31,485 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:35,534 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:35,535 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:35,536 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:35,537 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:35,544 - openevolve.process_parallel - WARNING - Iteration 90 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 13933 input tokens (20000 > 30000 - 13933). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:35,572 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:35,572 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:35,846 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:35,847 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:36,551 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:36,552 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:40,643 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:40,644 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:40,917 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:40,917 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:41,618 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:41,619 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:41,620 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:41,621 - openevolve.process_parallel - WARNING - Iteration 91 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:41,622 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:41,655 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:41,655 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:45,714 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:45,715 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:45,983 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:45,984 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:45,985 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:45,987 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:45,990 - openevolve.process_parallel - WARNING - Iteration 92 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:46,020 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:46,020 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:46,723 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:46,724 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:50,800 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:50,801 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:50,802 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:50,804 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:50,815 - openevolve.process_parallel - WARNING - Iteration 93 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 17804 input tokens (20000 > 30000 - 17804). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:50,838 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:50,839 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:51,086 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:51,087 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:51,792 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:51,793 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:55,879 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:55,879 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:56,161 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:56,162 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:19:56,864 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:56,865 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:56,866 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:56,867 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:19:56,872 - openevolve.process_parallel - WARNING - Iteration 94 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:19:56,900 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:19:56,901 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:00,951 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:00,952 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:01,232 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:01,233 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:01,233 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:01,235 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:20:01,243 - openevolve.process_parallel - WARNING - Iteration 95 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:01,269 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:01,269 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:01,971 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:01,972 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:06,021 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:06,021 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:06,022 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:06,024 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:20:06,031 - openevolve.process_parallel - WARNING - Iteration 96 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:06,057 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:06,058 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:06,329 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:06,330 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:07,038 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:07,039 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:11,098 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:11,098 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:11,404 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:11,405 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:12,109 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:12,110 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:12,111 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:12,114 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
2025-12-10 23:20:12,116 - openevolve.process_parallel - WARNING - Iteration 97 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:12,148 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:12,148 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:16,167 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:16,168 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:16,470 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:16,471 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:16,472 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:16,477 - openevolve.process_parallel - WARNING - Iteration 98 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:17,217 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:17,218 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:21,238 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:21,238 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:21,239 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:21,242 - openevolve.process_parallel - WARNING - Iteration 99 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16384 input tokens (20000 > 30000 - 16384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:22,287 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:22,288 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-10 23:20:27,355 - httpx - INFO - HTTP Request: POST http://localhost:2993/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 23:20:27,356 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:27,357 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:27,361 - openevolve.process_parallel - WARNING - Iteration 100 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 20000. This model's maximum context length is 30000 tokens and your request has 16383 input tokens (20000 > 30000 - 16383). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 23:20:27,361 - openevolve.process_parallel - INFO - âœ… Evolution completed - Maximum iterations reached
2025-12-10 23:20:27,380 - openevolve.database - INFO - Saved database with 7 programs to openevolve_output/checkpoints/checkpoint_100
2025-12-10 23:20:27,383 - openevolve.controller - INFO - Saved best program at checkpoint 100 with metrics: combined_score=0.5533
2025-12-10 23:20:27,383 - openevolve.controller - INFO - Saved checkpoint at iteration 100 to openevolve_output/checkpoints/checkpoint_100
2025-12-10 23:20:27,419 - openevolve.process_parallel - INFO - Stopped process pool
2025-12-10 23:20:27,419 - openevolve.controller - INFO - Using tracked best program: 9006210d-00ef-4b0e-9fd6-4f62acf62563
2025-12-10 23:20:27,419 - openevolve.controller - INFO - Evolution complete. Best program has metrics: combined_score=0.5533
2025-12-10 23:20:27,423 - openevolve.controller - INFO - Saved best program to openevolve_output/best/best_program.py with program info to openevolve_output/best/best_program_info.json
