{"id": "04a2a38a-954b-4996-afed-062e1004c972", "code": "from dataclasses import dataclass\n\n@dataclass\nclass Triple:\n    subject: str\n    predicate: str\n    object: str\n\n# EVOLVE-BLOCK-START\n\ndef predict(triples: list[Triple]) -> str:\n    \"\"\"\n    Generates a natural language sentence from a list of triples.\n    \"\"\"\n    sentence = \"\"\n    subject = \"\"\n    for triple in triples:\n        if not subject:\n            subject = triple.subject\n            sentence += f\"{subject} \"\n\n        if triple.predicate == \"cityServed\":\n            sentence += f\"serves the city of {triple.object}. \"\n        elif triple.predicate == \"elevationAboveTheSeaLevel\":\n            sentence += f\"is at an elevation of {triple.object} meters above sea level. \"\n        elif triple.predicate == \"location\":\n            sentence += f\"is located in {triple.object}. \"\n        elif triple.predicate == \"country\":\n            country = triple.object\n            if \"capital\" in [t.predicate for t in triples if t.subject == country]:\n                capital_triple = next(t for t in triples if t.subject == country and t.predicate == \"capital\")\n                sentence += f\"is in {country}, where the capital is {capital_triple.object}. \"\n            else:\n                sentence += f\"is in {triple.object}. \"\n        elif triple.predicate == \"capital\":\n            sentence += f\"is the capital of {next(t.subject for t in triples if t.predicate == 'country' and t.object == triple.object)}. \"\n        elif triple.predicate == \"iataLocationIdentifier\":\n            sentence += f\"has an IATA identifier of {triple.object}. \"\n        elif triple.predicate == \"icaoLocationIdentifier\":\n            sentence += f\"has an ICAO identifier of {triple.object}. \"\n        elif triple.predicate == \"runwayLength\":\n            sentence += f\"has a runway length of {triple.object} meters. \"\n        elif triple.predicate == \"runwayName\":\n            sentence += f\"The runway name is {triple.object}. \"\n        else:\n            if triple.predicate == \"isPartOf\":\n                sentence += f\"is part of {triple.object} which \"\n            elif triple.predicate == \"language\":\n                sentence += f\"speaks {triple.object}. \"\n            elif triple.predicate == \"operatingOrganisation\":\n                sentence += f\"is operated by {triple.object}. \"\n            else:\n                sentence += f\"has a {triple.predicate} of {triple.object}. \"\n\n    return sentence.strip()\n\n# EVOLVE-BLOCK-END", "language": "python", "parent_id": "b8afdbba-d7b9-47f3-9e70-81a8bfdf95d2", "generation": 6, "timestamp": 1768413267.3312607, "iteration_found": 256, "metrics": {"combined_score": 0.4509885545012393}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 19 lines with 3 lines", "parent_metrics": {"combined_score": 0.44916998933086205}, "island": 1}, "prompts": {"diff_user": {"system": "You are an expert in fixing your collegues code. You know that the code should be in format:\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly, indentations and endlines as well)\n=======\n# New replacement code\n>>>>>>> REPLACE\nProvided the incorrect format of SEARCH/REPLACE fix it to a correct format.\n        ", "user": "An incorrect diff format was detected in this change:\n```diff\n<<<<<<< SEARCH\n        elif triple.predicate == \"runwayName\":\n            runway_name = triple.object\n            runway_length = None\n            runway_surface = None\n            for other_triple in triples:\n                if other_triple.subject == triple.subject and other_triple.predicate == \"runwayLength\":\n                    runway_length = other_triple.object\n                elif other_triple.subject == triple.subject and other_triple.predicate == \"runwaySurfaceType\":\n                    runway_surface = other_triple.object\n\n            if runway_length and runway_surface:\n                sentence += f\"has a runway named {runway_name} with a length of {runway_length} meters and a surface of {runway_surface}. \"\n            elif runway_length:\n                sentence += f\"has a runway named {runway_name} with a length of {runway_length} meters. \"\n            elif runway_surface:\n                sentence += f\"has a runway named {runway_name} with a surface of {runway_surface}. \"\n            else:\n                sentence += f\"has a runway named {runway_name}. \"\n        else:\n=======\n        elif triple.predicate == \"runwayName\":\n            sentence += f\"The runway name is {triple.object}. \"\n        else:\n```\nPlease fix it to the correct format.", "responses": ["```diff\n<<<<<<< SEARCH\n        elif triple.predicate == \"runwayName\":\n            runway_name = triple.object\n            runway_length = None\n            runway_surface = None\n            for other_triple in triples:\n                if other_triple.subject == triple.subject and other_triple.predicate == \"runwayLength\":\n                    runway_length = other_triple.object\n                elif other_triple.subject == triple.subject and other_triple.predicate == \"runwaySurfaceType\":\n                    runway_surface = other_triple.object\n\n            if runway_length and runway_surface:\n                sentence += f\"has a runway named {runway_name} with a length of {runway_length} meters and a surface of {runway_surface}. \"\n            elif runway_length:\n                sentence += f\"has a runway named {runway_name} with a length of {runway_length} meters. \"\n            elif runway_surface:\n                sentence += f\"has a runway named {runway_name} with a surface of {runway_surface}. \"\n            else:\n                sentence += f\"has a runway named {runway_name}. \"\n        else:\n=======\n        elif triple.predicate == \"runwayName\":\n            sentence += f\"The runway name is {triple.object}. \"\n        else:\n>>>>>>> REPLACE\n```"]}}, "artifacts_json": "{\"poor_program_score_0\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAarhus Airport | runwayLength | 2777.0\\nThe generated text was:\\nAarhus Airport has a runway length of 2777.0 meters.\\nThe example correct sentences are:\\nThe runway length at Aarhus Airport is 2777.0 meters.\\nThe runway length at Aarhus airport is 2777.0.\\n\", \"poor_program_score_1\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAarhus Airport | runwayName | \\\"10L/28R\\\"\\nThe generated text was:\\nAarhus Airport The runway name is \\\"10L/28R\\\".\\nThe example correct sentences are:\\nAarhus Airport runway name is 10L/28R.\\n10L/28R is the runway name of the Aarhus Airport.\\nThe runway name of Aarhus Airport is 10L/28R.\\n\", \"poor_program_score_2\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAbilene Regional Airport | 1stRunwayLengthFeet | 3678\\nThe generated text was:\\nAbilene Regional Airport has a 1stRunwayLengthFeet of 3678.\\nThe example correct sentences are:\\nThe length of the 1st runway at Abilene Regional airport is 3678 feet.\\n\", \"poor_program_score_3\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAbilene Regional Airport | 1stRunwaySurfaceType | Asphalt\\nThe generated text was:\\nAbilene Regional Airport has a 1stRunwaySurfaceType of Asphalt.\\nThe example correct sentences are:\\nThe first runway at Abilene Regional Airport is made from asphalt.\\nThe 1st runway at Abilene Regional Airport is made of Asphalt.\\n\", \"poor_program_score_4\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAbilene Regional Airport | 3rdRunwayLengthFeet | 7202\\nThe generated text was:\\nAbilene Regional Airport has a 3rdRunwayLengthFeet of 7202.\\nThe example correct sentences are:\\nThe third runway at Abilene Regional Airport is 7,202 feet long.\\nThe 3rd runway at Abilene Regional airport is 7202 feet.\\nThe Abilene Regional Airport's 3rd runway length is ft is 7202.\\n\", \"poor_program_score_5\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAbilene Regional Airport | icaoLocationIdentifier | \\\"KABI\\\"\\nThe generated text was:\\nAbilene Regional Airport has an ICAO identifier of \\\"KABI\\\".\\nThe example correct sentences are:\\nAbilene Regional Airport ICAO Location Identifier is KABI.\\nKABI is the ICAO location identifier of Abilene Regional Airport.\\nThe ICAO Location Identifier of Abilene Regional Airport is KABI.\\n\", \"poor_program_score_6\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAbilene Regional Airport | runwayLength | 2194.0\\nThe generated text was:\\nAbilene Regional Airport has a runway length of 2194.0 meters.\\nThe example correct sentences are:\\nThe runway length of Abilene Regional Airport is 2194.0.\\n\", \"poor_program_score_7\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAbilene Regional Airport | runwayLength | 2195.0\\nThe generated text was:\\nAbilene Regional Airport has a runway length of 2195.0 meters.\\nThe example correct sentences are:\\nThe runway length of Abilene Regional Airport is 2,195.\\nThe runway length of Abilene Regional Airport is 2195.0.\\n\", \"poor_program_score_8\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAbilene Regional Airport | runwayName | \\\"17L/35R\\\"\\nThe generated text was:\\nAbilene Regional Airport The runway name is \\\"17L/35R\\\".\\nThe example correct sentences are:\\nAbilene Regional Airport runway name is 17L/35R.\\n17L/35R is the runway name of Abilene Regional Airport.\\nAbilene Regional Airport has the runway name 17L/35R.\\n\", \"poor_program_score_9\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAbilene Regional Airport | runwayName | \\\"17R/35L\\\"\\nThe generated text was:\\nAbilene Regional Airport The runway name is \\\"17R/35L\\\".\\nThe example correct sentences are:\\n17R/35L is the runway name at Abilene Regional airport.\\nThe name of the runway at Abilene Regional Airport is 17R/35L.\\nThe runway name of Abilene Regional Airport is 17R/35L.\\n\", \"poor_program_score_10\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAdirondack Regional Airport | 1stRunwayLengthFeet | 6573\\nThe generated text was:\\nAdirondack Regional Airport has a 1stRunwayLengthFeet of 6573.\\nThe example correct sentences are:\\nThe length of the first runway at Adirondack Regional Airport is 6,573 feet.\\n6573 feet is the length of the first runway at Adirondack Regional Airport.\\nThe 1st runway length in feet of Adirondack Regional Airport is 6573.\\n\", \"poor_program_score_11\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAdirondack Regional Airport | locationIdentifier | \\\"SLK\\\"\\nThe generated text was:\\nAdirondack Regional Airport has a locationIdentifier of \\\"SLK\\\".\\nThe example correct sentences are:\\nAdirondack Regional Airport location identifier is SLK.\\nSLK is the I.D. of the Adirondack Regional Airport.\\nThe Adirondack Regional Airport location identifier is SLK.\\n\", \"poor_program_score_12\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAdirondack Regional Airport | runwayLength | 1219.0\\nThe generated text was:\\nAdirondack Regional Airport has a runway length of 1219.0 meters.\\nThe example correct sentences are:\\nThe runway length of Adirondack Regional Airport is 1,219.\\nThe runway length at Adirondack Regional Airport is 1219.0.\\nThe runway length of Adirondack Regional Airport is 1219.0.\\n\", \"poor_program_score_13\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAdirondack Regional Airport | runwayLength | 2003.0\\nThe generated text was:\\nAdirondack Regional Airport has a runway length of 2003.0 meters.\\nThe example correct sentences are:\\nThe runway length of Adirondack Regional Airport is 2003.0.\\nThe length of the runway at Adirondack Regional Airport is 2003.0.\\n\", \"poor_program_score_14\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAdolfo Su\\u00e1rez Madrid\\u2013Barajas Airport | runwayName | \\\"18R/36L\\\"\\nThe generated text was:\\nAdolfo Su\\u00e1rez Madrid\\u2013Barajas Airport The runway name is \\\"18R/36L\\\".\\nThe example correct sentences are:\\n18R/36L is the runway name of the Adolfo Su\\u00e1rez Madrid-Barajas Airport.\\n\", \"poor_program_score_15\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAgra Airport | iataLocationIdentifier | \\\"AGR\\\"\\nThe generated text was:\\nAgra Airport has an IATA identifier of \\\"AGR\\\".\\nThe example correct sentences are:\\nAgra Airport IATA Location Identifier is AGR.\\nAGR is the IATA I.D. of the Agra Airport.\\nAGR is the ATA Location Identifier for Agra Airport.\\n\", \"poor_program_score_16\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAgra Airport | runwayLength | 1818.0\\nThe generated text was:\\nAgra Airport has a runway length of 1818.0 meters.\\nThe example correct sentences are:\\n1818 is the length of the runway of Agra Airport.\\nThe length of the runway at Agra Airport is 1818.0.\\n\", \"poor_program_score_17\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAgra Airport | runwayLength | 2743.0\\nThe generated text was:\\nAgra Airport has a runway length of 2743.0 meters.\\nThe example correct sentences are:\\nAgra Airport's runway length is 2743.0.\\n\", \"poor_program_score_18\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAl-Taqaddum Air Base | locationIdentifier | \\\"MAT\\\"\\nThe generated text was:\\nAl-Taqaddum Air Base has a locationIdentifier of \\\"MAT\\\".\\nThe example correct sentences are:\\nThe location identifier for Al-Taqaddum air base is MAT.\\nThe location identifier of Al Taqaddum Air Base is MAT.\\n\", \"poor_program_score_19\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAl-Taqaddum Air Base | runwayLength | 4019.0\\nThe generated text was:\\nAl-Taqaddum Air Base has a runway length of 4019.0 meters.\\nThe example correct sentences are:\\nThe runway length of Al-Taqaddum Air Base is 4,019.\\nThe runway length of Al-Taqaddum air base is 4019.0.\\nThe runway length of Al Taqaddum Air Base is 4019.0.\\n\", \"poor_program_score_20\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAl Asad Airbase | icaoLocationIdentifier | \\\"ORAA\\\"\\nThe generated text was:\\nAl Asad Airbase has an ICAO identifier of \\\"ORAA\\\".\\nThe example correct sentences are:\\nORAA is the ICAO location identifier of Al Asad Airbase.\\nThe ICAO Location Identifier of Al Asad Airbase is ORAA.\\n\", \"poor_program_score_21\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAl Asad Airbase | runwayLength | 3090.0\\nThe generated text was:\\nAl Asad Airbase has a runway length of 3090.0 meters.\\nThe example correct sentences are:\\nThe runway length of Al Asad Airbase is 3090 meters.\\nThe runway length at Al Asad airbase is 3090.0.\\nAl Asad Airbase's runway length is 3090.0.\\n\", \"poor_program_score_22\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAl Asad Airbase | runwayLength | 3992.88\\nThe generated text was:\\nAl Asad Airbase has a runway length of 3992.88 meters.\\nThe example correct sentences are:\\nThe runway length of Al Asad Airbase is 3,992.88.\\n3992.88 is the length of the runway at Al Asad Airbase.\\n\", \"poor_program_score_23\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAl Asad Airbase | runwayName | \\\"08/26\\\"\\nThe generated text was:\\nAl Asad Airbase The runway name is \\\"08/26\\\".\\nThe example correct sentences are:\\n''08/26'' is the runway name at Al Asad Airbase.\\n08/26 is the runway name of the Al Asad Airbase.\\nThe runway name of Al Asad Airbase is 08/26.\\n\", \"poor_program_score_24\": \"The program did very poorly with BLEU score 0.0. The input triples were:\\nAl Asad Airbase | runwayName | \\\"09R/27L\\\"\\nThe generated text was:\\nAl Asad Airbase The runway name is \\\"09R/27L\\\".\\nThe example correct sentences are:\\n09R/27L is the runway name at Al Asad airbase.\\nThe name of the runway at al Asad Airbase is 09R/27L.\\nAl Asad Airbase has the runway name 09R/27L.\\n\"}", "artifact_dir": null, "embedding": null}