2025-12-04 00:54:29,677 - openevolve.controller - INFO - Logging to openevolve_output/logs/openevolve_20251204_005429.log
2025-12-04 00:54:29,682 - openevolve.controller - INFO - Set random seed to 42 for reproducibility
2025-12-04 00:54:29,721 - openevolve.llm.openai - INFO - Initialized OpenAI LLM with model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:54:29,721 - openevolve.llm.ensemble - INFO - Initialized LLM ensemble with models: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8 (weight: 1.00)
2025-12-04 00:54:29,738 - openevolve.prompt.sampler - INFO - Initialized prompt sampler
2025-12-04 00:54:29,739 - openevolve.prompt.sampler - INFO - Set custom templates: system=evaluator_system_message, user=None
2025-12-04 00:54:29,739 - openevolve.database - INFO - Initialized program database with 0 programs
2025-12-04 00:54:30,688 - openevolve.evaluator - INFO - Successfully loaded evaluation function from evaluator.py
2025-12-04 00:54:30,688 - openevolve.evaluator - WARNING - Configuration has 'cascade_evaluation: true' but evaluator 'evaluator.py' does not define 'evaluate_stage1' function. This will fall back to direct evaluation, making the cascade setting useless. Consider setting 'cascade_evaluation: false' or implementing cascade functions.
2025-12-04 00:54:30,688 - openevolve.evaluator - INFO - Initialized evaluator with evaluator.py
2025-12-04 00:54:30,688 - openevolve.controller - INFO - Initialized OpenEvolve with initial_program.py
2025-12-04 00:54:30,688 - openevolve.controller - INFO - Adding initial program to database
2025-12-04 00:54:38,561 - openevolve.evaluator - INFO - Evaluated program b0b616cb-9b0f-404d-a7d1-e9b56e9d5ef0 in 7.87s: combined_score=0.0000
2025-12-04 00:54:38,561 - openevolve.database - INFO - New MAP-Elites cell occupied in island 0: {'complexity': 5, 'diversity': 0}
2025-12-04 00:54:38,562 - openevolve.process_parallel - INFO - Initialized process parallel controller with 3 workers
2025-12-04 00:54:38,562 - openevolve.process_parallel - INFO - Started process pool with 3 processes
2025-12-04 00:54:38,562 - openevolve.controller - INFO - Using island-based evolution with 1 islands
2025-12-04 00:54:38,562 - openevolve.database - INFO - Island Status:
2025-12-04 00:54:38,562 - openevolve.database - INFO -  * Island 0: 1 programs, best=0.0000, avg=0.0000, diversity=0.00, gen=0 (best: b0b616cb-9b0f-404d-a7d1-e9b56e9d5ef0)
2025-12-04 00:54:38,562 - openevolve.process_parallel - INFO - Starting process-based evolution from iteration 1 for 10 iterations (total: 11)
2025-12-04 00:54:38,575 - openevolve.process_parallel - INFO - Early stopping disabled
2025-12-04 00:54:38,615 - openevolve.prompt.sampler - INFO - Set custom templates: system=evaluator_system_message, user=None
2025-12-04 00:54:38,615 - openevolve.prompt.sampler - INFO - Set custom templates: system=evaluator_system_message, user=None
2025-12-04 00:54:38,616 - openevolve.prompt.sampler - INFO - Set custom templates: system=evaluator_system_message, user=None
2025-12-04 00:54:39,507 - openevolve.evaluator - INFO - Successfully loaded evaluation function from evaluator.py
2025-12-04 00:54:39,507 - openevolve.evaluator - WARNING - Configuration has 'cascade_evaluation: true' but evaluator 'evaluator.py' does not define 'evaluate_stage1' function. This will fall back to direct evaluation, making the cascade setting useless. Consider setting 'cascade_evaluation: false' or implementing cascade functions.
2025-12-04 00:54:39,508 - openevolve.evaluator - INFO - Initialized evaluator with evaluator.py
2025-12-04 00:54:39,508 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:54:39,509 - openevolve.evaluator - INFO - Successfully loaded evaluation function from evaluator.py
2025-12-04 00:54:39,510 - openevolve.evaluator - WARNING - Configuration has 'cascade_evaluation: true' but evaluator 'evaluator.py' does not define 'evaluate_stage1' function. This will fall back to direct evaluation, making the cascade setting useless. Consider setting 'cascade_evaluation: false' or implementing cascade functions.
2025-12-04 00:54:39,510 - openevolve.evaluator - INFO - Initialized evaluator with evaluator.py
2025-12-04 00:54:39,510 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:54:39,513 - openevolve.evaluator - INFO - Successfully loaded evaluation function from evaluator.py
2025-12-04 00:54:39,513 - openevolve.evaluator - WARNING - Configuration has 'cascade_evaluation: true' but evaluator 'evaluator.py' does not define 'evaluate_stage1' function. This will fall back to direct evaluation, making the cascade setting useless. Consider setting 'cascade_evaluation: false' or implementing cascade functions.
2025-12-04 00:54:39,514 - openevolve.evaluator - INFO - Initialized evaluator with evaluator.py
2025-12-04 00:54:39,514 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:54:39,726 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:39,726 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:39,726 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:39,727 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:39,727 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:39,727 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:44,766 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:44,767 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:44,778 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:44,778 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:44,779 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:44,779 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:49,807 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:49,808 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:49,817 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:49,817 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:49,818 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:49,818 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:54,848 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:54,849 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:54:54,849 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:54:54,850 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:54:54,855 - openevolve.process_parallel - WARNING - Iteration 3 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:54:54,867 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:54,867 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:54,868 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:54:54,868 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:54:54,868 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:54:54,869 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:54:54,869 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:54:54,869 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:54:54,876 - openevolve.process_parallel - WARNING - Iteration 1 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:54:54,876 - openevolve.process_parallel - WARNING - Iteration 2 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:54:54,893 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:54,894 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:54,904 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:54,904 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:54,904 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:54,904 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:59,934 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:59,934 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:59,943 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:59,943 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:54:59,944 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:54:59,944 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:04,976 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:04,976 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:04,984 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:04,984 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:04,985 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:04,985 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:10,015 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:10,016 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:10,017 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:10,017 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:55:10,017 - openevolve.process_parallel - WARNING - Iteration 4 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:10,034 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:10,034 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:10,034 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:10,034 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:10,035 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:10,035 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:10,035 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:55:10,035 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:55:10,038 - openevolve.process_parallel - WARNING - Iteration 5 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:10,038 - openevolve.process_parallel - WARNING - Iteration 6 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:10,063 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:10,063 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:10,070 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:10,070 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:10,079 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:10,080 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:15,106 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:15,109 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:15,114 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:15,114 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:15,115 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:15,115 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:20,149 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:20,150 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:20,158 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:20,158 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:20,159 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:20,159 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:25,190 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:25,191 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:25,191 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:25,192 - openevolve.llm.ensemble - INFO - Sampled model: RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8
2025-12-04 00:55:25,198 - openevolve.process_parallel - WARNING - Iteration 7 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:25,207 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:25,207 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:25,207 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:25,207 - openevolve.llm.openai - ERROR - All 4 attempts failed with error: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:25,208 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:25,208 - openevolve.process_parallel - ERROR - LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:25,209 - openevolve.process_parallel - WARNING - Iteration 8 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:25,209 - openevolve.process_parallel - WARNING - Iteration 9 error: LLM generation failed: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-04 00:55:25,215 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:25,215 - openevolve.llm.openai - WARNING - Error on attempt 1/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:30,241 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:30,241 - openevolve.llm.openai - WARNING - Error on attempt 2/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:35,268 - httpx - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-04 00:55:35,271 - openevolve.llm.openai - WARNING - Error on attempt 3/4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 10000. This model's maximum context length is 12000 tokens and your request has 4394 input tokens (10000 > 12000 - 4394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}. Retrying...
2025-12-04 00:55:37,494 - openevolve.controller - INFO - Received signal 2, initiating graceful shutdown...
2025-12-04 00:55:37,494 - openevolve.controller - INFO - Received signal 2, initiating graceful shutdown...
2025-12-04 00:55:37,494 - openevolve.controller - INFO - Received signal 2, initiating graceful shutdown...
2025-12-04 00:55:37,494 - openevolve.controller - INFO - Received signal 2, initiating graceful shutdown...
2025-12-04 00:55:37,495 - openevolve.process_parallel - INFO - Graceful shutdown requested...
2025-12-04 00:55:37,495 - openevolve.process_parallel - INFO - Graceful shutdown requested...
2025-12-04 00:55:37,495 - openevolve.process_parallel - INFO - Graceful shutdown requested...
2025-12-04 00:55:37,495 - openevolve.process_parallel - INFO - Graceful shutdown requested...
2025-12-04 00:55:37,500 - openevolve.process_parallel - INFO - Shutdown requested, canceling remaining evaluations...
2025-12-04 00:55:37,500 - openevolve.process_parallel - INFO - âœ… Evolution completed - Shutdown requested
2025-12-04 00:55:37,500 - openevolve.controller - INFO - Evolution stopped due to shutdown request
2025-12-04 00:55:38,346 - openevolve.controller - INFO - Force exit requested - terminating immediately
2025-12-04 00:55:38,346 - openevolve.controller - INFO - Force exit requested - terminating immediately
2025-12-04 00:55:38,346 - openevolve.controller - INFO - Force exit requested - terminating immediately
2025-12-04 00:55:38,346 - openevolve.controller - INFO - Force exit requested - terminating immediately
