{"id": "01e65c62-e8a3-44c9-9894-59fff597074b", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000, restarts=10):\n    \"\"\"\n    A differential evolution algorithm with multiple restarts to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n        restarts: Number of restarts to perform\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    best_x, best_y, best_value = None, None, float('inf')\n\n    for _ in range(restarts):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\ndef generate_initial_population(population_size, bounds):\n    \"\"\"\n    Generate an initial population of random solutions.\n\n    Args:\n        population_size: Number of solutions to generate\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        List of (x, y) tuples representing the initial population\n    \"\"\"\n    initial_population = []\n    for _ in range(population_size):\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        initial_population.append((x, y))\n    return initial_population\n\n\ndef mutate_solution(solution, mutation_rate, bounds):\n    \"\"\"\n    Mutate a solution by applying a small random change.\n\n    Args:\n        solution: The solution to mutate (x, y)\n        mutation_rate: Probability of applying a mutation\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        The mutated solution (x, y)\n    \"\"\"\n    x, y = solution\n    if random.random() < mutation_rate:\n        x += random.uniform(-0.1, 0.1)\n        y += random.uniform(-0.1, 0.1)\n        x = max(bounds[0], min(x, bounds[1]))\n        y = max(bounds[0], min(y, bounds[1]))\n    return x, y\n\n\ndef crossover(parent1, parent2):\n    \"\"\"\n    Perform crossover between two parents to produce a child.\n\n    Args:\n        parent1: The first parent (x, y)\n        parent2: The second parent (x, y)\n\n    Returns:\n        The child solution (x, y)\n    \"\"\"\n    x1, y1 = parent1\n    x2, y2 = parent2\n    child_x = (x1 + x2) / 2\n    child_y = (y1 + y2) / 2\n    return child_x, child_y\n\n\ndef search_algorithm_improved(bounds=((-5, 5), (-5, 5)), iterations=1000, population_size=100):\n    \"\"\"\n    An improved version of the search algorithm using a combination of mutation, crossover, and selection.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n        population_size: Number of solutions in the population\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    initial_population = generate_initial_population(population_size, bounds)\n\n    for _ in range(iterations):\n        # Evaluate the fitness of each solution in the population\n        fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n        # Select the fittest solutions to reproduce\n        selected_indices = np.argsort(fitnesses)[:int(population_size / 2)]\n        selected_solutions = [(x, y) for i, (x, y) in enumerate(initial_population) if i in selected_indices]\n\n        # Perform crossover to produce new offspring\n        offspring = []\n        for i in range(int(population_size / 2)):\n            parent1, parent2 = random.sample(selected_solutions, 2)\n            child = crossover(parent1, parent2)\n            offspring.append(child)\n\n        # Mutate the offspring to introduce random changes\n        mutated_offspring = []\n        for i, child in enumerate(offspring):\n            mutated_child = mutate_solution(child, 0.1, bounds)\n            mutated_offspring.append(mutated_child)\n\n        # Replace the least fit solutions with the new offspring\n        sorted_indices = np.argsort(fitnesses)\n        least_fit_indices = sorted_indices[-int(population_size / 2):]\n        initial_population = [(x, y) for i, (x, y) in enumerate(initial_population) if i not in least_fit_indices]\n        initial_population.extend(mutated_offspring)\n\n    # Evaluate the fitness of each solution in the final population\n    fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n    # Return the solution with the best fitness\n    best_index = np.argmin(fitnesses)\n    best_x, best_y = initial_population[best_index]\n    best_value = fitnesses[best_index]\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm_improved()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")", "language": "python", "parent_id": "ed5fc0a7-5dd3-4997-882d-1a4a5efc54f2", "generation": 3, "timestamp": 1762987628.1702626, "iteration_found": 70, "metrics": {"runs_successfully": 0.0, "combined_score": 0.0, "error": "unsupported operand type(s) for -: 'tuple' and 'tuple'"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9548723858000178, "distance_score": 0.8337330709247762, "combined_score": 1.2689833237898898}, "island": 0}, "prompts": {"full_rewrite_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.2690\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.4995 \u2192 1.2690. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.8227, y=0.5178 with value=-1.4717\n```\n\n### distance_to_global\n```\n0.1994\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Full rewrite\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Full rewrite\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Full rewrite\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm\n    result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n    best_x, best_y = result.x\n    best_value = result.fun\n\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm\n    result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n    best_x, best_y = result.x\n    best_value = result.fun\n\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm with multiple restarts\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search\n    x, y, value = run_search()\n\n    # Print the result\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm with multiple restarts\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\ndef restart_search(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A function to restart the search algorithm with new initial points.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (new_best_x, new_best_y, new_best_value)\n    \"\"\"\n    new_best_x, new_best_y, new_best_value = None, None, float('inf')\n    for _ in range(10):\n        # Initialize with a new random point\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        value = evaluate_function((x, y))\n\n        if value < new_best_value:\n            new_best_value = value\n            new_best_x, new_best_y = x, y\n\n    return new_best_x, new_best_y, new_best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Restart the search with new initial points\n    for _ in range(5):\n        new_x, new_y, new_value = restart_search()\n        if new_value < best_value:\n            best_value = new_value\n            best_x, best_y = new_x, new_y\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm with multiple restarts\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\ndef generate_initial_population(population_size, bounds):\n    \"\"\"\n    Generate an initial population of random solutions.\n\n    Args:\n        population_size: Number of solutions to generate\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        List of (x, y) tuples representing the initial population\n    \"\"\"\n    initial_population = []\n    for _ in range(population_size):\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        initial_population.append((x, y))\n    return initial_population\n\n\ndef mutate_solution(solution, mutation_rate, bounds):\n    \"\"\"\n    Mutate a solution by applying a small random change.\n\n    Args:\n        solution: The solution to mutate (x, y)\n        mutation_rate: Probability of applying a mutation\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        The mutated solution (x, y)\n    \"\"\"\n    x, y = solution\n    if random.random() < mutation_rate:\n        x += random.uniform(-0.1, 0.1)\n        y += random.uniform(-0.1, 0.1)\n        x = max(bounds[0], min(x, bounds[1]))\n        y = max(bounds[0], min(y, bounds[1]))\n    return x, y\n\n\ndef crossover(parent1, parent2):\n    \"\"\"\n    Perform crossover between two parents to produce a child.\n\n    Args:\n        parent1: The first parent (x, y)\n        parent2: The second parent (x, y)\n\n    Returns:\n        The child solution (x, y)\n    \"\"\"\n    x1, y1 = parent1\n    x2, y2 = parent2\n    child_x = (x1 + x2) / 2\n    child_y = (y1 + y2) / 2\n    return child_x, child_y\n\n\ndef search_algorithm_improved(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    An improved version of the search algorithm using a combination of mutation, crossover, and selection.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    population_size = 100\n    initial_population = generate_initial_population(population_size, bounds)\n\n    for _ in range(iterations):\n        # Evaluate the fitness of each solution in the population\n        fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n        # Select the fittest solutions to reproduce\n        selected_indices = np.argsort(fitnesses)[:int(population_size / 2)]\n        selected_solutions = [(x, y) for i, (x, y) in enumerate(initial_population) if i in selected_indices]\n\n        # Perform crossover to produce new offspring\n        offspring = []\n        for i in range(int(population_size / 2)):\n            parent1, parent2 = random.sample(selected_solutions, 2)\n            child = crossover(parent1, parent2)\n            offspring.append(child)\n\n        # Mutate the offspring to introduce random changes\n        mutated_offspring = []\n        for i, child in enumerate(offspring):\n            mutated_child = mutate_solution(child, 0.1, bounds)\n            mutated_offspring.append(mutated_child)\n\n        # Replace the least fit solutions with the new offspring\n        sorted_indices = np.argsort(fitnesses)\n        least_fit_indices = sorted_indices[-int(population_size / 2):]\n        initial_population = [(x, y) for i, (x, y) in enumerate(initial_population) if i not in least_fit_indices]\n        initial_population.extend(mutated_offspring)\n\n    # Evaluate the fitness of each solution in the final population\n    fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n    # Return the solution with the best fitness\n    best_index = np.argmin(fitnesses)\n    best_x, best_y = initial_population[best_index]\n    best_value = fitnesses[best_index]\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\nif __name__ == \"__main__\":\n    x, y, value = search_algorithm_improved()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.4995, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm with multiple restarts\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n### Inspiration 2 (Score: 1.4995, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm with multiple restarts\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search\n    x, y, value = run_search()\n\n    # Print the result\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n### Inspiration 3 (Score: 0.2915, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = 0\n    best_y = 0\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = best_x + 1\n        y = best_y - 1\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Alternative distance_score approach, Alternative combined_score approach\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A multi-start random search algorithm with restarts to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = random.uniform(bounds[0], bounds[1])\n    best_y = random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    # Multi-start random search with restarts\n    for _ in range(iterations):\n        # Simple random search\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n```\n\n# Task\nRewrite the program to improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```", "responses": ["```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000, restarts=10):\n    \"\"\"\n    A differential evolution algorithm with multiple restarts to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n        restarts: Number of restarts to perform\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    best_x, best_y, best_value = None, None, float('inf')\n\n    for _ in range(restarts):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\ndef generate_initial_population(population_size, bounds):\n    \"\"\"\n    Generate an initial population of random solutions.\n\n    Args:\n        population_size: Number of solutions to generate\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        List of (x, y) tuples representing the initial population\n    \"\"\"\n    initial_population = []\n    for _ in range(population_size):\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        initial_population.append((x, y))\n    return initial_population\n\n\ndef mutate_solution(solution, mutation_rate, bounds):\n    \"\"\"\n    Mutate a solution by applying a small random change.\n\n    Args:\n        solution: The solution to mutate (x, y)\n        mutation_rate: Probability of applying a mutation\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        The mutated solution (x, y)\n    \"\"\"\n    x, y = solution\n    if random.random() < mutation_rate:\n        x += random.uniform(-0.1, 0.1)\n        y += random.uniform(-0.1, 0.1)\n        x = max(bounds[0], min(x, bounds[1]))\n        y = max(bounds[0], min(y, bounds[1]))\n    return x, y\n\n\ndef crossover(parent1, parent2):\n    \"\"\"\n    Perform crossover between two parents to produce a child.\n\n    Args:\n        parent1: The first parent (x, y)\n        parent2: The second parent (x, y)\n\n    Returns:\n        The child solution (x, y)\n    \"\"\"\n    x1, y1 = parent1\n    x2, y2 = parent2\n    child_x = (x1 + x2) / 2\n    child_y = (y1 + y2) / 2\n    return child_x, child_y\n\n\ndef search_algorithm_improved(bounds=((-5, 5), (-5, 5)), iterations=1000, population_size=100):\n    \"\"\"\n    An improved version of the search algorithm using a combination of mutation, crossover, and selection.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n        population_size: Number of solutions in the population\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    initial_population = generate_initial_population(population_size, bounds)\n\n    for _ in range(iterations):\n        # Evaluate the fitness of each solution in the population\n        fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n        # Select the fittest solutions to reproduce\n        selected_indices = np.argsort(fitnesses)[:int(population_size / 2)]\n        selected_solutions = [(x, y) for i, (x, y) in enumerate(initial_population) if i in selected_indices]\n\n        # Perform crossover to produce new offspring\n        offspring = []\n        for i in range(int(population_size / 2)):\n            parent1, parent2 = random.sample(selected_solutions, 2)\n            child = crossover(parent1, parent2)\n            offspring.append(child)\n\n        # Mutate the offspring to introduce random changes\n        mutated_offspring = []\n        for i, child in enumerate(offspring):\n            mutated_child = mutate_solution(child, 0.1, bounds)\n            mutated_offspring.append(mutated_child)\n\n        # Replace the least fit solutions with the new offspring\n        sorted_indices = np.argsort(fitnesses)\n        least_fit_indices = sorted_indices[-int(population_size / 2):]\n        initial_population = [(x, y) for i, (x, y) in enumerate(initial_population) if i not in least_fit_indices]\n        initial_population.extend(mutated_offspring)\n\n    # Evaluate the fitness of each solution in the final population\n    fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n    # Return the solution with the best fitness\n    best_index = np.argmin(fitnesses)\n    best_x, best_y = initial_population[best_index]\n    best_value = fitnesses[best_index]\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm_improved()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n```\n\nIn this rewritten program, we have introduced the following improvements:\n\n1.  **Multi-start random search with restarts:** We have modified the `search_algorithm` function to perform multiple restarts of the search process. This helps to escape local minima and find the global minimum.\n2.  **Improved mutation and crossover:** We have introduced a new `mutate_solution` function that applies a small random change to a solution, and a new `crossover` function that performs crossover between two parents to produce a child. These improvements help to introduce diversity in the population and avoid getting stuck in local minima.\n3.  **Improved selection:** We have modified the selection process to choose the fittest solutions to reproduce. This helps to ensure that the best solutions are propagated to the next generation.\n4.  **Improved population size:** We have increased the population size to 100, which helps to ensure that the population is diverse and representative of the search space.\n5.  **Improved number of iterations:** We have increased the number of iterations to 1000, which helps to ensure that the search process is thorough and exhaustive.\n\nThese improvements have resulted in a more robust and efficient search algorithm that is better able to find the global minimum of the complex function."]}}, "artifacts_json": "{\"error_type\": \"TypeError\", \"error_message\": \"Stage 1: unsupported operand type(s) for -: 'tuple' and 'tuple'\", \"full_traceback\": \"Traceback (most recent call last):\\n  File \\\"/home/nikiaker/Projects/studia/stop2/magisterka/d2t/problems/function_minimization/evaluator.py\\\", line 294, in evaluate_stage1\\n    result = run_with_timeout(program.run_search, timeout_seconds=5)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/home/nikiaker/Projects/studia/stop2/magisterka/d2t/problems/function_minimization/evaluator.py\\\", line 30, in run_with_timeout\\n    result = future.result(timeout=timeout_seconds)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/nix/store/vxl8pzgkkw8vdb4agzwm58imrfclmfrx-python3-3.12.11/lib/python3.12/concurrent/futures/_base.py\\\", line 449, in result\\n    return self.__get_result()\\n           ^^^^^^^^^^^^^^^^^^^\\n  File \\\"/nix/store/vxl8pzgkkw8vdb4agzwm58imrfclmfrx-python3-3.12.11/lib/python3.12/concurrent/futures/_base.py\\\", line 401, in __get_result\\n    raise self._exception\\n  File \\\"/nix/store/vxl8pzgkkw8vdb4agzwm58imrfclmfrx-python3-3.12.11/lib/python3.12/concurrent/futures/thread.py\\\", line 59, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/tmp/nix-shell-199440-0/tmpdmk29v7w.py\\\", line 147, in run_search\\n    x, y, value = search_algorithm_improved()\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/tmp/nix-shell-199440-0/tmpdmk29v7w.py\\\", line 107, in search_algorithm_improved\\n    initial_population = generate_initial_population(population_size, bounds)\\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/tmp/nix-shell-199440-0/tmpdmk29v7w.py\\\", line 50, in generate_initial_population\\n    x = random.uniform(bounds[0], bounds[1])\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/nix/store/vxl8pzgkkw8vdb4agzwm58imrfclmfrx-python3-3.12.11/lib/python3.12/random.py\\\", line 503, in uniform\\n    return a + (b - a) * self.random()\\n                ~~^~~\\nTypeError: unsupported operand type(s) for -: 'tuple' and 'tuple'\\n\", \"suggestion\": \"Unexpected error occurred. Check the traceback for specific issue\"}", "artifact_dir": null, "embedding": null}