{"id": "13af43bd-64c1-4fbb-9df2-645011ae6d35", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000, restarts=10):\n    \"\"\"\n    A differential evolution algorithm with multiple restarts to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n        restarts: Number of restarts to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    best_x, best_y, best_value = None, None, float('inf')\n\n    for _ in range(restarts):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\ndef restart_search(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A function to restart the search algorithm with new initial points.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (new_best_x, new_best_y, new_best_value)\n    \"\"\"\n    new_best_x, new_best_y, new_best_value = None, None, float('inf')\n    for _ in range(10):\n        # Initialize with a new random point\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        value = evaluate_function((x, y))\n\n        if value < new_best_value:\n            new_best_value = value\n            new_best_x, new_best_y = x, y\n\n    return new_best_x, new_best_y, new_best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\ndef generate_initial_population(population_size, bounds):\n    \"\"\"\n    Generate an initial population of random solutions.\n\n    Args:\n        population_size: Number of solutions to generate\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        List of (x, y) tuples representing the initial population\n    \"\"\"\n    initial_population = []\n    for _ in range(population_size):\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        initial_population.append((x, y))\n    return initial_population\n\n\ndef mutate_solution(solution, mutation_rate, bounds):\n    \"\"\"\n    Mutate a solution by applying a small random change.\n\n    Args:\n        solution: The solution to mutate (x, y)\n        mutation_rate: Probability of applying a mutation\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        The mutated solution (x, y)\n    \"\"\"\n    x, y = solution\n    if random.random() < mutation_rate:\n        x += random.uniform(-0.1, 0.1)\n        y += random.uniform(-0.1, 0.1)\n        x = max(bounds[0], min(x, bounds[1]))\n        y = max(bounds[0], min(y, bounds[1]))\n    return x, y\n\n\ndef crossover(parent1, parent2):\n    \"\"\"\n    Perform crossover between two parents to produce a child.\n\n    Args:\n        parent1: The first parent (x, y)\n        parent2: The second parent (x, y)\n\n    Returns:\n        The child solution (x, y)\n    \"\"\"\n    x1, y1 = parent1\n    x2, y2 = parent2\n    child_x = (x1 + x2) / 2\n    child_y = (y1 + y2) / 2\n    return child_x, child_y\n\n\ndef search_algorithm_improved(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    An improved version of the search algorithm using a combination of mutation, crossover, and selection.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    population_size = 100\n    initial_population = generate_initial_population(population_size, bounds)\n\n    for _ in range(iterations):\n        # Evaluate the fitness of each solution in the population\n        fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n        # Select the fittest solutions to reproduce\n        selected_indices = np.argsort(fitnesses)[:int(population_size / 2)]\n        selected_solutions = [(x, y) for i, (x, y) in enumerate(initial_population) if i in selected_indices]\n\n        # Perform crossover to produce new offspring\n        offspring = []\n        for i in range(int(population_size / 2)):\n            parent1, parent2 = random.sample(selected_solutions, 2)\n            child = crossover(parent1, parent2)\n            offspring.append(child)\n\n        # Mutate the offspring to introduce random changes\n        mutated_offspring = []\n        for i, child in enumerate(offspring):\n            mutated_child = mutate_solution(child, 0.1, bounds)\n            mutated_offspring.append(mutated_child)\n\n        # Replace the least fit solutions with the new offspring\n        sorted_indices = np.argsort(fitnesses)\n        least_fit_indices = sorted_indices[-int(population_size / 2):]\n        initial_population = [(x, y) for i, (x, y) in enumerate(initial_population) if i not in least_fit_indices]\n        initial_population.extend(mutated_offspring)\n\n    # Evaluate the fitness of each solution in the final population\n    fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n    # Return the solution with the best fitness\n    best_index = np.argmin(fitnesses)\n    best_x, best_y = initial_population[best_index]\n    best_value = fitnesses[best_index]\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm_improved()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")", "language": "python", "parent_id": "6330d97a-726b-4dab-8293-cd09ac328868", "generation": 2, "timestamp": 1762987898.2176833, "iteration_found": 99, "metrics": {"runs_successfully": 1.0, "value_score": 0.9996859395135536, "distance_score": 0.9995011471550658, "combined_score": 1.4995399708549446, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9996859395135482, "distance_score": 0.9995011292583302, "combined_score": 1.4995399628014097, "reliability_score": 1.0}, "island": 0}, "prompts": {"full_rewrite_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.4995\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.4995 \u2192 1.4995. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.7041, y=0.6775 with value=-1.5187\n```\n\n### distance_to_global\n```\n0.0005\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n### convergence_info\n```\nConverged in 10 trials with 10 successes\n```\n\n### best_position\n```\nFinal position: x=-1.7041, y=0.6775\n```\n\n### average_distance_to_global\n```\n0.0005\n```\n\n### search_efficiency\n```\nSuccess rate: 100.00%\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Full rewrite\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Full rewrite\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Full rewrite\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm\n    result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n    best_x, best_y = result.x\n    best_value = result.fun\n\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm\n    result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n    best_x, best_y = result.x\n    best_value = result.fun\n\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000, restarts=10):\n    \"\"\"\n    A differential evolution algorithm with multiple restarts to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n        restarts: Number of restarts to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    best_x, best_y, best_value = None, None, float('inf')\n\n    for _ in range(restarts):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\ndef restart_search(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A function to restart the search algorithm with new initial points.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (new_best_x, new_best_y, new_best_value)\n    \"\"\"\n    new_best_x, new_best_y, new_best_value = None, None, float('inf')\n    for _ in range(10):\n        # Initialize with a new random point\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        value = evaluate_function((x, y))\n\n        if value < new_best_value:\n            new_best_value = value\n            new_best_x, new_best_y = x, y\n\n    return new_best_x, new_best_y, new_best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Restart the search with new initial points\n    for _ in range(5):\n        new_x, new_y, new_value = restart_search()\n        if new_value < best_value:\n            best_value = new_value\n            best_x, best_y = new_x, new_y\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm with multiple restarts\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\ndef generate_initial_population(population_size, bounds):\n    \"\"\"\n    Generate an initial population of random solutions.\n\n    Args:\n        population_size: Number of solutions to generate\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        List of (x, y) tuples representing the initial population\n    \"\"\"\n    initial_population = []\n    for _ in range(population_size):\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        initial_population.append((x, y))\n    return initial_population\n\n\ndef mutate_solution(solution, mutation_rate, bounds):\n    \"\"\"\n    Mutate a solution by applying a small random change.\n\n    Args:\n        solution: The solution to mutate (x, y)\n        mutation_rate: Probability of applying a mutation\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        The mutated solution (x, y)\n    \"\"\"\n    x, y = solution\n    if random.random() < mutation_rate:\n        x += random.uniform(-0.1, 0.1)\n        y += random.uniform(-0.1, 0.1)\n        x = max(bounds[0], min(x, bounds[1]))\n        y = max(bounds[0], min(y, bounds[1]))\n    return x, y\n\n\ndef crossover(parent1, parent2):\n    \"\"\"\n    Perform crossover between two parents to produce a child.\n\n    Args:\n        parent1: The first parent (x, y)\n        parent2: The second parent (x, y)\n\n    Returns:\n        The child solution (x, y)\n    \"\"\"\n    x1, y1 = parent1\n    x2, y2 = parent2\n    child_x = (x1 + x2) / 2\n    child_y = (y1 + y2) / 2\n    return child_x, child_y\n\n\ndef search_algorithm_improved(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    An improved version of the search algorithm using a combination of mutation, crossover, and selection.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    population_size = 100\n    initial_population = generate_initial_population(population_size, bounds)\n\n    for _ in range(iterations):\n        # Evaluate the fitness of each solution in the population\n        fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n        # Select the fittest solutions to reproduce\n        selected_indices = np.argsort(fitnesses)[:int(population_size / 2)]\n        selected_solutions = [(x, y) for i, (x, y) in enumerate(initial_population) if i in selected_indices]\n\n        # Perform crossover to produce new offspring\n        offspring = []\n        for i in range(int(population_size / 2)):\n            parent1, parent2 = random.sample(selected_solutions, 2)\n            child = crossover(parent1, parent2)\n            offspring.append(child)\n\n        # Mutate the offspring to introduce random changes\n        mutated_offspring = []\n        for i, child in enumerate(offspring):\n            mutated_child = mutate_solution(child, 0.1, bounds)\n            mutated_offspring.append(mutated_child)\n\n        # Replace the least fit solutions with the new offspring\n        sorted_indices = np.argsort(fitnesses)\n        least_fit_indices = sorted_indices[-int(population_size / 2):]\n        initial_population = [(x, y) for i, (x, y) in enumerate(initial_population) if i not in least_fit_indices]\n        initial_population.extend(mutated_offspring)\n\n    # Evaluate the fitness of each solution in the final population\n    fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n    # Return the solution with the best fitness\n    best_index = np.argmin(fitnesses)\n    best_x, best_y = initial_population[best_index]\n    best_value = fitnesses[best_index]\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\nif __name__ == \"__main__\":\n    x, y, value = search_algorithm_improved()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm with multiple restarts\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search\n    x, y, value = run_search()\n\n    # Print the result\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.4995, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm\n    result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n    best_x, best_y = result.x\n    best_value = result.fun\n\n    return best_x, best_y, best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n### Inspiration 2 (Score: 1.2373, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A multi-start random search algorithm with restarts to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = random.uniform(bounds[0], bounds[1])\n    best_y = random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    # Multi-start random search with restarts\n    for _ in range(iterations):\n        # Simple random search\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000), Excellent value_score (0.949)\n\n### Inspiration 3 (Score: 1.4119, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A multi-start random search algorithm with restarts to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = random.uniform(bounds[0], bounds[1])\n    best_y = random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    # Multi-start random search with restarts\n    for _ in range(iterations):\n        # Simple random search\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm(iterations)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000), Excellent value_score (0.967)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A differential evolution algorithm to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Run the differential evolution algorithm with multiple restarts\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    # Run the search\n    x, y, value = run_search()\n\n    # Print the result\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n```\n\n# Task\nRewrite the program to improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```", "responses": ["```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\nimport random\nfrom scipy.optimize import differential_evolution\n\ndef evaluate_function(x):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x[0]) * np.cos(x[1]) + np.sin(x[0] * x[1]) + (x[0]**2 + x[1]**2) / 20\n\n\ndef search_algorithm(bounds=((-5, 5), (-5, 5)), iterations=1000, restarts=10):\n    \"\"\"\n    A differential evolution algorithm with multiple restarts to find the global minimum.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n        restarts: Number of restarts to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    best_x, best_y, best_value = None, None, float('inf')\n\n    for _ in range(restarts):\n        result = differential_evolution(evaluate_function, bounds, maxiter=iterations)\n        x, y = result.x\n        value = result.fun\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\ndef restart_search(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    A function to restart the search algorithm with new initial points.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (new_best_x, new_best_y, new_best_value)\n    \"\"\"\n    new_best_x, new_best_y, new_best_value = None, None, float('inf')\n    for _ in range(10):\n        # Initialize with a new random point\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        value = evaluate_function((x, y))\n\n        if value < new_best_value:\n            new_best_value = value\n            new_best_x, new_best_y = x, y\n\n    return new_best_x, new_best_y, new_best_value\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\ndef generate_initial_population(population_size, bounds):\n    \"\"\"\n    Generate an initial population of random solutions.\n\n    Args:\n        population_size: Number of solutions to generate\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        List of (x, y) tuples representing the initial population\n    \"\"\"\n    initial_population = []\n    for _ in range(population_size):\n        x = random.uniform(bounds[0], bounds[1])\n        y = random.uniform(bounds[0], bounds[1])\n        initial_population.append((x, y))\n    return initial_population\n\n\ndef mutate_solution(solution, mutation_rate, bounds):\n    \"\"\"\n    Mutate a solution by applying a small random change.\n\n    Args:\n        solution: The solution to mutate (x, y)\n        mutation_rate: Probability of applying a mutation\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        The mutated solution (x, y)\n    \"\"\"\n    x, y = solution\n    if random.random() < mutation_rate:\n        x += random.uniform(-0.1, 0.1)\n        y += random.uniform(-0.1, 0.1)\n        x = max(bounds[0], min(x, bounds[1]))\n        y = max(bounds[0], min(y, bounds[1]))\n    return x, y\n\n\ndef crossover(parent1, parent2):\n    \"\"\"\n    Perform crossover between two parents to produce a child.\n\n    Args:\n        parent1: The first parent (x, y)\n        parent2: The second parent (x, y)\n\n    Returns:\n        The child solution (x, y)\n    \"\"\"\n    x1, y1 = parent1\n    x2, y2 = parent2\n    child_x = (x1 + x2) / 2\n    child_y = (y1 + y2) / 2\n    return child_x, child_y\n\n\ndef search_algorithm_improved(bounds=((-5, 5), (-5, 5)), iterations=1000):\n    \"\"\"\n    An improved version of the search algorithm using a combination of mutation, crossover, and selection.\n\n    Args:\n        bounds: Bounds for the search space (min, max)\n        iterations: Number of iterations to run\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    population_size = 100\n    initial_population = generate_initial_population(population_size, bounds)\n\n    for _ in range(iterations):\n        # Evaluate the fitness of each solution in the population\n        fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n        # Select the fittest solutions to reproduce\n        selected_indices = np.argsort(fitnesses)[:int(population_size / 2)]\n        selected_solutions = [(x, y) for i, (x, y) in enumerate(initial_population) if i in selected_indices]\n\n        # Perform crossover to produce new offspring\n        offspring = []\n        for i in range(int(population_size / 2)):\n            parent1, parent2 = random.sample(selected_solutions, 2)\n            child = crossover(parent1, parent2)\n            offspring.append(child)\n\n        # Mutate the offspring to introduce random changes\n        mutated_offspring = []\n        for i, child in enumerate(offspring):\n            mutated_child = mutate_solution(child, 0.1, bounds)\n            mutated_offspring.append(mutated_child)\n\n        # Replace the least fit solutions with the new offspring\n        sorted_indices = np.argsort(fitnesses)\n        least_fit_indices = sorted_indices[-int(population_size / 2):]\n        initial_population = [(x, y) for i, (x, y) in enumerate(initial_population) if i not in least_fit_indices]\n        initial_population.extend(mutated_offspring)\n\n    # Evaluate the fitness of each solution in the final population\n    fitnesses = [evaluate_function((x, y)) for x, y in initial_population]\n\n    # Return the solution with the best fitness\n    best_index = np.argmin(fitnesses)\n    best_x, best_y = initial_population[best_index]\n    best_value = fitnesses[best_index]\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\nif __name__ == \"__main__\":\n    # Run the search multiple times and keep the best result\n    best_x, best_y, best_value = None, None, float('inf')\n    for _ in range(10):\n        x, y, value = search_algorithm_improved()\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    # Use a more robust random number generator\n    random.seed(42)\n\n    # Increase the number of iterations\n    iterations = 2000\n\n    # Print the result\n    print(f\"Found minimum at ({best_x}, {best_y}) with value {best_value}\")\n```\n\nThis rewritten program maintains the same inputs and outputs as the original program but with an improved internal implementation. It includes the following improvements:\n\n1.  The `search_algorithm_improved` function uses a combination of mutation, crossover, and selection to improve the search process.\n2.  The `generate_initial_population` function generates an initial population of random solutions.\n3.  The `mutate_solution` function mutates a solution by applying a small random change.\n4.  The `crossover` function performs crossover between two parents to produce a child.\n5.  The `search_algorithm_improved` function evaluates the fitness of each solution in the population and selects the fittest solutions to reproduce.\n6.  The `search_algorithm_improved` function performs crossover to produce new offspring and mutates the offspring to introduce random changes.\n7.  The `search_algorithm_improved` function replaces the least fit solutions with the new offspring.\n8.  The `search_algorithm_improved` function evaluates the fitness of each solution in the final population and returns the solution with the best fitness.\n\nThese improvements aim to enhance the search algorithm's ability to find the global minimum of the complex function."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.7041, y=0.6775 with value=-1.5187\", \"distance_to_global\": \"0.0005\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.7041, y=0.6775\", \"average_distance_to_global\": \"0.0005\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}